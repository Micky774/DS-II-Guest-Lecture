{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deriving hypothesis tests\n",
    "    - Hypothesis testing\n",
    "        - z-score\n",
    "        - p-value\n",
    "        - power\n",
    "        - Null Hypothesis (1-sample t-test)\n",
    "        - Alternative Hypothesis (2-sample t-test)\n",
    "    - multi-hypothesis regularization (including one-sided vs two-sided experiment hygiene)\n",
    "    - bootstrap methods\n",
    "- Deriving permutation tests\n",
    "    - monte-carlo methods\n",
    "    - general permutation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Scientest's Toolbox\n",
    "\n",
    "Q: What is the point of statistics (WFR)?\n",
    "\n",
    "Folks have myriad \"interpretations\" of statistics, and there are competing overall philosophies (frequentist vs. Bayesian) but ultimatley it boils down to something along the lines of figuring out how extreme certain events are. That's the most agnostic way I could describe it. It turns out that this problem, and its study, unlock a huge amount of tools for use in very complex, nuanced situations. A lot of cutting-edge science is firmly laid on the foundations of advanced statistics. Today I'm going to be talking about some of the under-discussed parts of statistics that often get used in these cutting-edge cases, in the hopes that whatever kind of research you do, you'll be more prepared to make it rigorous.\n",
    "\n",
    "#### Basic Probability Reminder\n",
    "\n",
    "We can talk about statistics as a formal analysis of probabilities (though that is itself an oversimplification), so let's cover probability again really quickly. When we talk about probability, we are really talking about the probability of **events**. What is the event in the following probability question: What is the probability of getting three heads in a row when flipping a coin (WFR)? Are there any other events with the same probability?\n",
    "\n",
    "You may have heard about the two main schools of thought when it comes to probability: frequentist and Bayesian. I'll talk just a bit about them, but the content of today's lecture is largely agnostic of them. Consider drawing a single card from a shuffled deck. Let's say you set it aside face down. What is the probability that the card you drew is, say, an ace of spades? A Bayesian would say 1/52, since the act of drawing and setting it aside did not give you any new information on the card, therefore it is the same as had you not drawn it yet at all. A frequentist would say exactly 0 or exactly 1, since the randomness has already been resolved when you drew that card, and so now all that remains is to see whether or not it is the ace of spades. Nobody is always a frequentist or Bayesian -- these are two *perspectives* that largely converge to the same math, but are useful in different situations.\n",
    "\n",
    "#### What is a Random Variable (RV)?\n",
    "\n",
    "Let's consider rolling a fair six-sided die. Specifically, let's talk about the immediate *next* roll that has not occured yet. The result of the roll is considered a \"random variable\". It is \"random\" in the sense that it has not occured yet, and we expect that there is variation in its outcome (i.e. it is non-deterministic to the extent of our understanding). Past rolls were each once a random variable, but as soon as they are resolved / observed, they are now just static data. That's an important transition. All probability is discussed from the perspective of analyzing RVs, since observed data has probability either 0 or 1. Note that a function of an RV is, itself, a new RV. This allows transformations of a certain kind through what are known as *measurable* functions (most functions you can write simply). Let's briefly talk about the \"formal\" structure of an RV, but we won't linger on it for too long.\n",
    "\n",
    "$$\n",
    "X: \\mathcal{B}\\rightarrow\\mathbb{R}\n",
    "$$\n",
    "\n",
    "So, our next roll is an RV. What can we say about it (WFR)? These insights do not come from the RV itself. Where do they come from (WFR)? This is what we refer to as the \"distribution\" of the RV -- specifically a probability density function for continuous RVs. Knowing the distribution allows us to explicitly calculate the probability of arbitrary events.\n",
    "\n",
    "$$\n",
    "f:\\mathbb{R}\\rightarrow\\mathbb{R} \\,|\\, \\mathcal{E}\\subset\\mathcal{B} \\\\\n",
    "Pr(X\\in\\mathcal{E}) = \\int_{\\mathcal{E}} f(x) dx = \\int_\\mathcal{B} I_{x\\in\\mathcal{E}}f(x) dx\n",
    "$$\n",
    "\n",
    "An RV and its distribution are *different things* but often used interchangeably since all we can really describe concretely are either an RVs observed values, or its distribution (the former feeds into the latter). You can think of an RV as the upstream source of both the distribution, and observed values. I mentioned earlier that functions of RVs are themselves RVs, but is there a way to get rid of RVs? What if we just want concrete numbers to discuss? Observation is one tool, but it's not really enough. We can use the above definition of a pdf to *evaluate* the probability of particular events. In a more general sense, the main tool is *integration*, and *expected value* (or *expectation*) in particular.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\int_\\mathbb{R} xf(x) dx\n",
    "$$\n",
    "\n",
    "Let's get back to our die-roll RV. What can we learn about its distribution if we record, say, 100 rolls (WFR)? These are *statistics*, which are in practice most often computed from empirical observations of an RV and are thus termed *estimators* since the *true* statistic is computed over the entire distribution. Similarly, we can talk about statistics at the population level, and at the sample level. Most of the time, we use estimators and sample statistics to try to understand the underlying population statistics. \n",
    "\n",
    "\n",
    "Here's the interesting thing though: since functions of RVs are themselves RVs, these statistics and their estimators are RVs, with respect to the underlying distribution. This means that to actually evaluate them, we need to consider the tools at our disposal. We can *observe* them, or *integrate* them. When working with empirical data, computing statistics by using those observed data points is equivalent to \"observing\" the *estimate*. Why is this not the same as observing the population statistic itself?\n",
    "\n",
    "\n",
    "We'll talk more about other ways to understand statistics in the later sections dedicated to **Monte Carlo** methods and **Bootstrapping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a hypothesis?\n",
    "\n",
    "You've all heard of \"hypothesis tests\" but what are statistical hypothesis and why do we need them? Why is not enough to talk about the exact quantities of statistics? Why do we need to bring in what can sometimes sound like petty semantics? The answer to all of that is **context**. Numbers without context mean nothing. Similarly, insights without context are no insights at all. If you're testing a new algorithm to predict cancer stages in adults, then getting 65% accuracy doesn't immediately scream good or awful by itself. Knowing that you can get 95% accuracy with a model that only ever spits out \"no cancer\" as its guess plainly illumanites that maybe your algorithm with a 65% accuracy needs to be revised a bit more...\n",
    "\n",
    "A hypothesis is, in essence, our human wisdom being compiled into a salient assumption about the data we are about to observe. Perhaps the most common case of hypothesis testing for students is the idea of a two-sample test. Given two distinct samples, can you make a claim about whether they were drawn from the same distribution or not? You could reflexively skip the hypothesis framing part, and start spitting out the statistics of the samples -- their means, medians, standard deviations, etc. -- but that doesn't actually give you an answer. The tricky part of continuous distributions is that as long as their domains overlap, there is **always** a chance that two samples were generated by the same distribution. You can never fully disqualify that possibility.\n",
    "\n",
    "But just because it is possible, doesn't mean it's probable. That's where hypothesis testing comes in. We understand that we can never be free from that possibility, and in fact we respect that it is in a sense a natural \"default\" assumption. It's the simplest assumption, and a fairly safe conservative one to make. Thus we regard it as our default hypothesis, or **null hypothesis**. Now we need to compare its likelihood to that of another hypothesis. This one we refer to as the **alternative hypothesis**. Generally, well-designed hypothesis tests will exhibit the pattern where the null and alternative hypothesis are somewhat complementary, but this can almost never be fully guaranteed.\n",
    "\n",
    "The next question that arises is then \"how do we calculate the likelihoods of different hypothesis?\" The rough process is as follows: generate an experimental procedure guaranteeing we sample from the correct distributions (which may or may not be the same), generate a sample, evaluate relevant statistics on those samples, and compare how extreme (read: unlikely) those statistics would be under the null hypothesis. If the statistic is something that you could reasonably see being generated by a sample from the null distribution, then we don't have much reason to believe that a *different* distribution is to blame. If, however, the statistic is almost impossible to generate under the null distribution, it gives us a large degree of confidence in claiming that it must have been generated by something else (namely our alternative hypothesis).\n",
    "\n",
    "#### Extreme values and probability\n",
    "\n",
    "We are about to get to the crux of most applied statistics: *quantifying* how *extreme* certain statistics are, or would be. What do we mean by how *extreme* a statistic is? Simply put, it is the probability that the statistic (as an RV) would be observed at its current value, or greater (or lesser, or both, depending on the hypothesis being tested). This requires an idea of what the *usual* distribution of that statistic would be, so that we talk about how *unusual* the particular value is. This, however, requires knowing the exact distribution which is impossible since *that* requires knowing the generating distributions for the samples. So, are we stuck?\n",
    "\n",
    "The open question is then \"how do we guess the distributions of these statistics under the null hypothesis?\" The simplest way is to use any assumptions you have about the null hypothesis. Perhaps we have assumed that it is a Gaussian distribution with a certain mean and variance. Maybe we know the mean, but not the variance. Maybe we know neither, or don't even know it is Gaussian! Perhaps we don't know its general shape, but know its mean and/or variance. Each one of these slightly different starting assumptions results in distinct, unique hypothesis testing procedures -- that's why there are tens of them when you look online. These are often what are called **parametric** tests, in that they assume something about the shape or properties of the distribution. In the rarer but not uncommon case where you can't even know that, one must turn to **non-parametric** options, which we'll talk more about in the **Monte Carlo** and **Bootstrapping** sections. As a small spoiler, the final quantification for how \"extreme\" a statistic is under a null distribution is referred to as a \"p-value\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholds and Error\n",
    "\n",
    "But suppose you have all that figured out, there still remains the problem of determining how \"extreme\" is \"extreme\" enough to raise some flags. Different people may be ready to jump to conclusions at different thresholds! I mean, on the two extreme ends you could have folks that -- regardless of the statistic -- will *always* reject the null hypothesis, or *never* reject the null hypothesis. I posit that sanity lies somewhere between them. To that end, we often talk about type I and type II errors, or false-positives and false-negatives respectively. These correspond to either incorrectly rejecting the null hypothesis, or incorrectly accepting it, respectively. We can decide our cut-offs in terms of these quantities. Generally, we will optimize for one of them, since you generally can't drive both down to zero simultaneously.\n",
    "\n",
    "In most cases, folks prefer to optimize for type I errors, since the null hypothesis is often formulated as a sort of safer, conservative default. That means we control for our chances of false-discovery (it's safer to *not* believe that everything causes cancer, unless very strictly proven otherwise). This false-discovery chance is actually exactly the $\\alpha$ that you often see in hypothesis tests. Why is that exactly?\n",
    "\n",
    "\n",
    "#### P-values and the Uniform distribution\n",
    "\n",
    "Suppose you're trying to use a statistic $T$ to evaluate the viability of an alternative hypothesis that claims that a sample set of $n$ samples were generated under distribution $X$, whereas the null-hypothesis is that they were generated under distribution $Y$. In an abstract sense, until we compute exact values, $T$ is an RV, which means we can run some math on it. Specifically, we are going to investigate what we can say about $T$ under the null hypothesis of $Y$ (which I'll denote $T_Y$ as a reminder), as well as how we could maybe try to design a test that has a fixed false-discovery rate of $alpha$. Since we're talking about how extreme an observed value of $t$ may be, perhaps it would make sense to consider what $T_Y$ as a distribution means. For a given observed value $t$, $T_Y$ encodes the information of how extreme $t$ is. We can describe $Pr(T_Y<t)$, which I will denote as $F_T(t)$, i.e. the CDF of $T_Y$ evaluated at $t$ as the \"extremeness\" of an observed statistic $t$. In fact, let's define $p=F_T(t)$ and $P=F_T(T)$ as the p-value of the observed statistic, and the distribution of p-values under the null hypothesis respectively.\n",
    "\n",
    "Looking a bit closer, can we make any claims about the distribution of $P$? Let's start by trying to figure out its CDF. Specifically \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\n",
    "F_P(p) &= Pr(P < p) \\\\\n",
    "&= Pr(F_T(T) < p) \\\\\n",
    "&= Pr(T < F_T^{-1}(p)) \\\\\n",
    "&= F_T(F_T^{-1}(p)) \\\\\n",
    "&= p \\\\\n",
    "\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This result shows that **p-values are uniformally distributed under the null-hypothesis**. This is obvious in hindsight when looking back at the definition of the p-value, but is a fact that gets lost upon most people. This is the crux of all hypothesis testing. What makes this such an impactful result in practice? Well, it provides us an easy way to determine the threshold we should set for the statistic $T$ based on our desired false-discovery rate $\\alpha$. Specifically, a false-discovery rate $\\alpha$ is achieved when $Pr(p < p_\\alpha) = \\alpha$. So, what is $p_\\alpha$? Looking back at the proof earlier, we can solve for it as follows: $Pr(p < p_\\alpha) = p_\\alpha$ therefore $p_\\alpha = \\alpha$. It feels like nothing really happened here, and that's the brilliance of the p-value formalization. We get this kind of control for free. We are now looking for statistics that generate p-values less than $\\alpha$. These values represent the *percentile* of the observed statistic under the null hypothesis. If we observe such a statistic, then we may *reject the null hypothesis* in favor of the alternative, knowing that the probability of incorrectly rejecting the null-hypothesis is $\\alpha$.\n",
    "\n",
    "Note in practice, $\\alpha$ is usually set to $0.05$ however this is not always the case and shouldn't be blindly assumed. In the most extreme cases, for example particle physics, tests set $\\alpha ~= 2.87e-7$. You must **consciously decide on an alpha level before running any experiments** in order to avoid corrupting results. We will discuss more about that in the **p-hacking** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a hypothesis test\n",
    "\n",
    "Let's start with a simple parametric example where we have two sample sets from what we suspect may be normal distributions with equal variance, but differing means. This is a common case and is often taught in most intro stat classes. You could use several tests here including a Z-test, student's t-test, or Welch's t-test. The exact choice depends on the exact set of assumptions.\n",
    "\n",
    "For now, however, let's make our own test. Suppose we have a fixed normal distribution with known population mean and population variance that describes the error in machining precision for a certain machine in a factory. We susepct that it has recently degraded, and now the error follows a different distribution. We must decide on: a *statistic* to measure for this test, a false-discovery rate to target, a *null-hypothesis* under which we determine the statistic's distribution, as well as an alternative hypothesis. Our null hypothesis is very simply that the sample is distributed under $\\mathcal{N}(\\mu_0, \\sigma^2)$. Our alternative hypothesis, since we are not concerned about our machine becoming *more* precise (a decrease in error), will focus on the case of it becoming *less* precise (an increase in error). Thus our alternative hypothesis is that our sample is distributed under $\\mu > \\mu_0$ where $\\mu, \\mu_0$ are the alternative-hypothesis and null-hypothesis means respectively. For simplicity, let's use the sample mean as the basis of our statistic. Sample mean is defined as $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i$. Since are determining whether $\\mu > \\mu_0$, our actual statistic will be $T=\\bar{X} - \\mu_0$.  Now we need to figure out what our statistic's distribution (that is to say $F_{\\bar{X}}$) looks like under the null hypothesis, so that we can determine how extreme our observed statistic is when evaluating a sample set.\n",
    "\n",
    "That is to say, given $\\{X_i\\}_{i=1}^n\\sim\\mathcal{N}(\\mu_0, \\sigma^2)$ then what is the distribution of $\\bar{X}$? By the Central Limit Theorem, we have that $\\sqrt{n}(\\bar{X}-\\mu_0)\\rightarrow\\mathcal{N}(0, \\sigma^2)$ thus $\\bar{X}\\sim\\mathcal{N}(\\mu_0, \\frac{\\sigma^2}{n})$. Notice that this is not the same as the null-distribution we started with. This is the *distribution of the statistic under the null hypothesis*. Now we can obtain a formula for the p-value under this setup as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\n",
    "p &= F_{\\bar{X}}(\\bar{x}) \\\\\n",
    "&=Pr(\\bar{X} < \\bar{x}) \\\\\n",
    "&=Pr(Z < (\\bar{x}- \\mu_0)\\frac{\\sqrt{n}}{\\sigma} ) \\,|\\,Z\\sim\\mathcal{N}(0, 1)\\\\\n",
    "&=\\Phi((\\bar{x}- \\mu_0)\\frac{\\sqrt{n}}{\\sigma}) \\\\\n",
    "&=\\frac{1}{2}\\lbrack1+\\operatorname{erf}(\\frac{\\bar{x}-\\mu_0}{\\sigma}\\sqrt{\\frac{n}{2}})\\rbrack\n",
    "\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, we could use this test fairly easily. I want to point out that although it is -- relatively speaking -- simple, and maybe even intuitive, it is not a fantastic test. In particular, it has a lower **power** compared to other tests, where *power* is defined as the chance of *correctly rejecting the null hypothesis when it is false*. It is the \"dual\" of the false-discovery rate, in that it is the true-acceptance rate. Optimizing for one usually occurs at the expense of the other assuming you keep the same test in place. Different tests, however, have different powers *at the same false-discovery rate*. Thus power is often used as a measure of the quality of a test. We won't be going too far into power here, but it is a fantastic concept and aids in the design and selection of hypothesis tests. I would absolutely recommend folks read into it on their own if they want to make the most out of practical statistics and feel somewhat lost regarding why there are so many different types of tests that almost seem redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Tests\n",
    "\n",
    "#### Numerical Methods\n",
    "\n",
    "I mentioned earlier that there are a few ways to resolve RVs into deterministic values. We've been working directly with observational methods, but that's not the only tool at our disposal. Often times, the most complex statistics *cannot* rely solely on observational methods, and must turn to numeric integration methods, at the heart of which lies the concept of Monte Carlo methods, and the aptly-named Law of the unconscious statistician (LOTUS). Let's start by motivating a use case. Suppose you have an RV $X$. How would you find the expected value $\\mathbb{E}[X]$ if all you could do was sample from $X$ and evaluate the pdf at the points sampled? What if the analytical formula were too complicated to work with, or perhaps entirely unknown but you were still able to sample from the distribution and compute pointwise density values? This is a fairly common use case in real world statistics. With these assumptions, let's take Kullback–Leibler Divergence as an example. It is a *statistical distance* between two distributions, given by:\n",
    "$$\n",
    "D_{KL}(P||Q) = \\int{P(x)\\operatorname{ln}(\\frac{P(x)}{Q(x)})dx}\n",
    "$$\n",
    "\n",
    "As you can imagine, the formula for that gets very complicated, very quickly, as you move away from perfect ideal distributions. In the real world, distributions can be very messy, and quantities such as KL divergence become a pipe dream to solve analytically. In fact, even just lower bounds to KL divergence are incredibly cherished by the field, and are a perpetual area of research. So, if it is so difficult to calculate, how are we going to do it when all we have is the ability to sample from it, and the ability to evaluate the pdf at arbitrary points?\n",
    "\n",
    "For this, we rely on the so-called LOTUS. It was coined as such because the theorem statement itself is so simple and so intuitive that people mistook it as true by *definition* rather than it being something that needed to be non-trivially proved (which is the genuine case). Specifically, it states that the expected value of a function over an RV is equal to the integral/sum of the density/mass function of the RV times the function to be evaluated. In symbols:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g(X)]=\\int_\\mathbb{R}g(x)f(x)dx\n",
    "$$\n",
    "\n",
    "Evaluating for $g(x)=x$ we obtain the standard definition of the expected value. How does this solve our problem? Well, if we can formulate our statistics of interest as functions over our RV, then we can randomly sample the RV and *evaluate* the function over those samples, and take the average to estimate the statistic. Note that this is different than using observational data to generate sample estimates of population statistics, since *KL Divergence is a constant, not an RV$, and sample-wise estimates are much less stable due to the need to estimate the distributions themselves. The main difference in methods is the difference in assumptions and tools we have access to.\n",
    "\n",
    "How do we actually go from the definition of KL-divergence to a usable monte carlo estimate? Let's look at the (only) two pieces of the LOTUS. Specifically $g(x)$ and $\\int {\\cdot\\, f(x)dx}$. Integrating over the distribution pdf is equivalent to sampling repeatedly and taking the average of the integrand's value at each point. Thus we need only get our formula into a similar shape. In the definition of KL-divergence, $P,Q$ are the pdfs of their respective distributions, and could be written as $f_P, f_Q$. This makes it quite easy to take $g(x)=\\operatorname{ln}(\\frac{P(x)}{Q(x)})$ to satisfy \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\n",
    "D_{KL}(P || Q) &= \\int{P(x)\\operatorname{ln}(\\frac{P(x)}{Q(x)})dx} \\\\\n",
    "&= \\int{P(x)g(x)dx} \\\\\n",
    "&= \\mathbb{E}_{P}[g(X)]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### Monte Carlo Hypothesis Tests\n",
    "\n",
    "Let's combine what we've talked about so far. We have learned a lot about hypothesis tests, and their theoretical basis. We've learned about numerical methods, and how they allow us to avoid needing analytical answers to hard questions. Now, we will combine them. What if you want to employ a hypothesis test that you suspect will be effective, but is incredibly difficult (or outright impossible) to solve analytically? How can we utilize numerical methods to help us? Well, this is where simulation comes in handy.\n",
    "\n",
    "Starting with a *simple* complicated case, let's consider a sequence of coin tosses. The null hypothesis is that the sequence of coin tosses was generated by a coin with probability $\\theta$ of landing on Heads, and consequently $1-\\theta$ of landing on Tails. How do we test this hypothesis? How can we make it robust to antagonistic data samples?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
